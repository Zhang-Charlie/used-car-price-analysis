{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fd569eb-44f7-42ac-8d12-c9baa5bef380",
   "metadata": {},
   "source": [
    "# Notebook 1 \n",
    "## COMP30760 â€” Assignment 1  \n",
    "## Charlie Zhang - 23341901"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad6482d-773e-46f2-aa3a-0c13744df37e",
   "metadata": {},
   "source": [
    "## Task 1: Data Collection\n",
    "- Web scraping and parsing\n",
    "- Saving data in an appropriate format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55486c62-dcae-42ce-87b6-2aa17deb1261",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from urllib.parse import urljoin\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5d2dac-a475-4dcb-a546-a50175a95621",
   "metadata": {},
   "source": [
    "Download the HTML source code for the target web page at the link. This is where we will scrape all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd86d0a1-371a-4c02-b2cb-a3b0ffbf368b",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"\" // remove the url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24b11b4e-e27e-4da5-8593-e726fe492fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      "<head>\n",
      "  <meta charset=\"utf-8\">\n",
      "  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n",
      "  <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\n",
      "  <meta name=\"robots\" content=\"noindex\">  \n",
      "  <meta name=\"description\" content=\"Car sale records database for educational purposes. Browse second-hand car sales by manufacturer with detailed information including price, year, mileage, and specifications.\">\n",
      "  <title>Car Sale Records - Browse Used Car Sales Database</title>\n",
      "  <link rel=\"stylesheet\" href=\"http://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css\">\n",
      "  <script src=\"https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js\"></script>\n",
      "  <script src=\"http://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js\"></script>\n",
      "  <link rel=\"stylesheet\" type=\"text/css\" href=\"style.css\">\n",
      "</head>\n",
      "<body>\n",
      "    <div class=\"container\">\n",
      "      <main>\n",
      "        <section class=\"instructions\">\n",
      "          <div class=\"row\">\n",
      "            <div class=\"col-md-12\">\n",
      "              <h1>Car Sale Records Database</h1>\n",
      "              <p class=\"lead\">\n",
      "                Browse comprehensive listings of second-hand car sales across Ireland. Each listing includes detailed information to help you make informed decisions.\n",
      "              </p>\n",
      "              \n",
      "              <h2>What Information Is Available</h2>\n",
      "              <ul class=\"info-list\">\n",
      "                <li>Make and model of the vehicle</li>\n",
      "                <li>Sale date and price in Euros</li>\n",
      "                <li>Manufacturing year and mileage</li>\n",
      "                <li>Vehicle classification (hatchback, SUV, saloon, etc.)</li>\n",
      "                <li>Transmission type (manual or automatic)</li>\n",
      "                <li>Fuel type (petrol, diesel, hybrid, etc.)</li>\n",
      "                <li>Sale location within Ireland</li>\n",
      "                <li>Additional features and vehicle history</li>\n",
      "              </ul>\n",
      "            </div>\n",
      "          </div>\n",
      "        </section>\n",
      "\n",
      "        <section class=\"manufacturer-list\">\n",
      "          <div class=\"row\">\n",
      "            <div class=\"col-md-12\">\n",
      "              <h2>Browse by Manufacturer</h2>\n",
      "              <p class=\"lead\">Select a car manufacturer to view all available sales records:</p>\n",
      "            </div>\n",
      "            \n",
      "            <div class=\"col-md-12\">\n",
      "              <div class=\"manufacturer-links\">\n",
      "                <div class=\"manufacturer-item\">\n",
      "                  <h4><a href=\"Audi-page01.html\">Make: Audi</a></h4>\n",
      "                  List of all car sales for Audi\n",
      "                </div>\n",
      "                \n",
      "                <div class=\"manufacturer-item\">\n",
      "                  <h4><a href=\"BMW-page01.html\">Make: BMW</a></h4>\n",
      "                  List of all car sales for BMW\n",
      "                </div>\n",
      "                \n",
      "                <div class=\"manufacturer-item\">\n",
      "                  <h4><a href=\"Mercedes-Benz-page01.html\">Make: Mercedes-Benz</a></h4>\n",
      "                  List of all car sales for Mercedes-Benz\n",
      "                </div>\n",
      "                \n",
      "                <div class=\"manufacturer-item\">\n",
      "                  <h4><a href=\"Volkswagen-page01.html\">Make: Volkswagen</a></h4>\n",
      "                  List of all car sales for Volkswagen\n",
      "                </div>\n",
      "                \n",
      "\n",
      "              </div>\n",
      "            </div>\n",
      "          </div>\n",
      "        </section>\n",
      "      </main>\n",
      "\n",
      "      <footer class=\"footer\">\n",
      "        <p>Content on this site is posted for educational purposes only.</p>\n",
      "      </footer>\n",
      "    </div>\n",
      "  </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import urllib.error\n",
    "\n",
    "try:\n",
    "    # open the url\n",
    "    response = urllib.request.urlopen(url)\n",
    "    # read the response data (bytes) and decode it into a string\n",
    "    html = response.read().decode(\"utf-8\", errors=\"replace\")\n",
    "    # print the html page\n",
    "    print(html)\n",
    "    # implement error handling\n",
    "except urllib.error.HTTPError as e:\n",
    "    print(f\"HTTP Error {e.code}: {e.reason}\")\n",
    "    html = None\n",
    "except urllib.error.URLError as e:\n",
    "    print(f\"Network Error: {e.reason}\")\n",
    "    html = None\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error: {e}\")\n",
    "    html = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bbc8c3-f49a-44cd-b1b5-6d5cd2368568",
   "metadata": {},
   "source": [
    "Find all the links on the page in order to find the makes of the cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7240cf9-823a-4715-bd1d-cc7881525ccc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 make links:\n",
      "- Make: Audi -> Audi-page01.html\n",
      "- Make: BMW -> BMW-page01.html\n",
      "- Make: Mercedes-Benz -> Mercedes-Benz-page01.html\n",
      "- Make: Volkswagen -> Volkswagen-page01.html\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "\n",
    "# create a BeautifulSoup object from the index page\n",
    "# using lxml because it is faster and reliable that the html.parser\n",
    "soup = bs4.BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "# from checking all of the links they all end with page01.html\n",
    "# these links are the make/brands of the cars\n",
    "# we use this to find all of the links with page01.html\n",
    "find_makes = [link for link in soup.find_all(\"a\", href=True) if \"page01.html\" in link[\"href\"]]\n",
    "\n",
    "\n",
    "# find the links and display how many are found and which ones were found\n",
    "if find_makes:\n",
    "    print(f\"Found {len(find_makes)} make links:\")\n",
    "    \n",
    "    for link in find_makes:\n",
    "        print(f\"- {link.get_text(strip=True)} -> {link['href']}\")\n",
    "        \n",
    "else: \n",
    "    # error handling - if there were none found\n",
    "    print(\"No links found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca1cd12-14e0-41b7-a42d-a5b26980eb52",
   "metadata": {},
   "source": [
    "Import all necessary objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7edacccd-94a1-47c1-aa9c-600ded40b3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "from urllib.parse import urljoin\n",
    "from io import StringIO\n",
    "\n",
    "import bs4\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b190430-aa26-4afd-836f-12f30a2005e2",
   "metadata": {},
   "source": [
    "Find the links with -page01.html as all of the pages of information have these in the ending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18060795-581d-4b21-9111-328987137859",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_links = [\n",
    "    (a[\"href\"].rsplit(\"-page01.html\", 1)[0], urljoin(url, a[\"href\"]))\n",
    "    for a in find_makes\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39eeda9-4215-48bd-8e1f-5b2c92e94810",
   "metadata": {},
   "source": [
    "Takes in raw html data and parsing the data. Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e42eaaeb-3fc7-4ece-9ded-e40823203ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_cars = []\n",
    "\n",
    "# this loop overs every make and find them because of the page01.html in the ending of the link\n",
    "for make, base_url in get_links:\n",
    "    prefix = base_url.rsplit(\"page01.html\", 1)[0]\n",
    "    page_no = 1\n",
    "    \n",
    "    # keeps looping until it scans all pages\n",
    "    while True:\n",
    "        page_url = f\"{prefix}page{page_no:02d}.html\"\n",
    "        try:\n",
    "            with urllib.request.urlopen(page_url) as resp:\n",
    "                page_html = resp.read().decode(\"utf-8\", errors=\"replace\") # decode the data into text\n",
    "\n",
    "        # error handling to show if it doesn't work\n",
    "        except urllib.error.HTTPError as e:\n",
    "            if e.code == 404:\n",
    "                break\n",
    "            print(\"HTTP error\", e.code, e.reason, \"for\", page_url)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(\"Fetch error:\", e, \"for\", page_url)\n",
    "            break\n",
    "\n",
    "        # 2nd soup for the page\n",
    "        page_soup = bs4.BeautifulSoup(page_html, \"lxml\")\n",
    "\n",
    "        # parse each table on the page using pandas.read_html\n",
    "        for table in page_soup.find_all(\"table\"):\n",
    "            \n",
    "            try:\n",
    "                t = pd.read_html(StringIO(str(table)))[0] # reads the html and turns it into pandas dataframe\n",
    "            except ValueError:\n",
    "                continue\n",
    "                \n",
    "            if t is None or t.empty:\n",
    "                continue\n",
    "\n",
    "            # assume first two columns are key and value\n",
    "            t = t.iloc[:, :2].copy()\n",
    "            t.columns = [\"field\", \"value\"]\n",
    "\n",
    "            # cleaning the data\n",
    "            t[\"field\"] = t[\"field\"].astype(str)\n",
    "            t[\"field\"] = t[\"field\"].str.strip()\n",
    "            t[\"field\"] = t[\"field\"].str.rstrip(\":\")\n",
    "\n",
    "            # more data cleaning\n",
    "            t[\"value\"] = t[\"value\"].astype(str)\n",
    "            t[\"value\"] = t[\"value\"].str.strip()\n",
    "\n",
    "            t = t[t[\"field\"].str.len() > 0]\n",
    "\n",
    "            # build a record dictionary because it make fields easily searchable\n",
    "            record = {}\n",
    "            for f, v in zip(t[\"field\"], t[\"value\"]):\n",
    "                if v and v.lower() != \"nan\":\n",
    "                    record[f] = v\n",
    "\n",
    "            # fixes case sensitive sale prices to a standard Sale Price\n",
    "            sales = t[\"field\"].str.strip().str.lower() == \"sale price\"\n",
    "            if sales.any():\n",
    "                record[\"Sale Price\"] = t.loc[sales, \"value\"].iloc[0]\n",
    "\n",
    "            # grabs the headings\n",
    "            title = table.find_previous([\"h1\", \"h2\", \"h3\"])\n",
    "            # save the title \n",
    "            if title:\n",
    "                record[\"Title\"] = title.get_text(\" \", strip=True)\n",
    "\n",
    "            # save the make\n",
    "            if record:\n",
    "                record[\"Make\"] = make\n",
    "                all_cars.append(record)\n",
    "\n",
    "        page_no += 1 # increments by 1 each time to move onto the next page\n",
    "        time.sleep(0.2) # added in a small delay to ensure data is collected safely "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a697159c-da3b-494e-9f53-ae0075dc1d94",
   "metadata": {},
   "source": [
    "Converts the scraped data into a pandas table and create a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4eaed0cf-7dff-4b85-b937-0790b587b8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cars_dataset.json created\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame.from_records(all_cars)\n",
    "df.to_json(\"cars_dataset.json\", orient=\"records\", indent=2, force_ascii=False) # Saves the table as a json\n",
    "\n",
    "print(\"cars_dataset.json created\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
